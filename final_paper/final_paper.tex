% Template for PLoS
% Version 3.5 March 2018
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended
% to minimize problems and delays during our production
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% Once your paper is accepted for publication,
% PLEASE REMOVE ALL TRACKED CHANGES in this file
% and leave only the final text of your manuscript.
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that
% no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file.
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission.
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.
%
% Do not include text that is not math in the math environment.
%
% Please add line breaks to long display equations when possible in order to fit size of the column.
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
% \usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace}
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
% \bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother



% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}


% Pandoc citation processing
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
% for Pandoc 2.8 to 2.10.1
\newenvironment{cslreferences}%
  {}%
  {\par}
% For Pandoc 2.11+
\newenvironment{CSLReferences}[3] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1 \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces\fi
  % set entry spacing
  \ifnum #2 > 0
  \setlength{\parskip}{#2\baselineskip}
  \fi
 }%
 {}
\usepackage{calc} % for calculating minipage widths
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}




\usepackage{forarray}
\usepackage{xstring}
\newcommand{\getIndex}[2]{
  \ForEach{,}{\IfEq{#1}{\thislevelitem}{\number\thislevelcount\ExitForEach}{}}{#2}
}

\setcounter{secnumdepth}{0}

\newcommand{\getAff}[1]{
  \getIndex{#1}{Smith College,StreetEasy}
}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Predicting Real Estate Prices Using Natural Language
Processing and Machine Learning
Algorithms} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Yipeng Lai\textsuperscript{\getAff{StreetEasy}},
Lauren Low\textsuperscript{\getAff{Smith College}},
Dayana Meza\textsuperscript{\getAff{Smith College}},
Emma Scott\textsuperscript{\getAff{Smith College}},
Elaine Ye\textsuperscript{\getAff{Smith College}},
Yanwan Zhu\textsuperscript{\getAff{Smith College}}\\
\bigskip
\textbf{\getAff{Smith College}}Smith College, 1 Chapin Way, Northampton,
MA 01063\\
\textbf{\getAff{StreetEasy}}StreetEasy, 1250 Broadway, New York, NY
10001\\
\bigskip
\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur eget
porta erat. Morbi consectetur est vel gravida pretium. Suspendisse ut
dui eu ante cursus gravida non sed sem. Nullam sapien tellus, commodo id
velit id, eleifend volutpat quam. Phasellus mauris velit, dapibus
finibus elementum vel, pulvinar non tellus. Nunc pellentesque pretium
diam, quis maximus dolor faucibus id. Nunc convallis sodales ante, ut
ullamcorper est egestas vitae. Nam sit amet enim ultrices, ultrices elit
pulvinar, volutpat risus.

% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step.
% Author Summary not valid for PLOS ONE submissions.
\section*{Author summary}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur eget
porta erat. Morbi consectetur est vel gravida pretium. Suspendisse ut
dui eu ante cursus gravida non sed sem. Nullam sapien tellus, commodo id
velit id, eleifend volutpat quam. Phasellus mauris velit, dapibus
finibus elementum vel, pulvinar non tellus. Nunc pellentesque pretium
diam, quis maximus dolor faucibus id. Nunc convallis sodales ante, ut
ullamcorper est egestas vitae. Nam sit amet enim ultrices, ultrices elit
pulvinar, volutpat risus.

\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Real estate is one of, if not the biggest, investments an individual
makes over the course of their lifetime. Traditionally, home valuation
has been conducted by professional property appraisers -- those licensed
and hired to give an opinion of a home's value, based on prices of
neighboring homes, property analysis and judgment. In recent years
though, there has been an uptick in developing models and algorithmic
resources to aid housing price estimates. Companies like Zillow, Trulia,
and others have developed machine learning models to automate the job of
an appraiser. These organizations take into account housing attributes
including property size, number of bedrooms, number of bathrooms,
geographic location, state of the economy, among other variables to
predict home prices.

StreetEasy, launched in 2006 and acquired by Zillow in 2013, is
reshaping the ways people buy, sell and rent property in New York City
and New Jersey. StreetEasy's existing predictive tool uses metrics on
bedrooms, bathrooms, property size, and geographic features to estimate
particular listing prices, similar to Zillow's Zestimate. One feature
the current machine learning algorithm neglects is the descriptive text
of given listings. A primary objective of this project is to convert
text-based listing descriptions into input variables for the purpose of
improving predictive model accuracy. In particular, we want to research
whether listing descriptions can be utilized to create meaningful
features to improve the accuracy of predicting home prices. Current
literature supports leveraging machine learning algorithms with natural
language processing to improve prediction accuracy {[}1,2{]}.

\hypertarget{research-questions-and-objectives}{%
\subsection{Research Questions and
Objectives}\label{research-questions-and-objectives}}

Some research questions include:

\begin{itemize}
\tightlist
\item
  Can we use listing attributes to predict home prices?
\item
  Can we convert the description text into meaningful features?
\item
  Can we use text-based features in addition to the existing features to
  predict home prices?
\end{itemize}

Many questions already have preliminary answers. In StreetEasy's current
model, home prices are predicted using variables such as number of
bedrooms, number of bathrooms, property size, time to subway, year the
property was built and property neighborhood/geographic coordinates. To
predict home prices more accurately, we include the listing description
text as tokens in the machine learning model. Using bag-of-words method
and word embeddings to extract keywords, our analysis has yielded some
key phrases such as ``stainless steel,'' ``washer dryer,'' ``windows,''
``storage,'' ``central park'' and ``closet space.''

In asking and answering these questions our goal is to improve upon
StreetEasy's existing machine learning algorithm by combining current
models with text-based analysis to predict home prices. By creating new
variables from the listing descriptions -- binary variables for the term
``stainless steel,'' say -- we aim to improve prediction accuracy.

\hypertarget{methods}{%
\section{Methods}\label{methods}}

\hypertarget{data-and-variables}{%
\subsection{Data and Variables}\label{data-and-variables}}

The data provided by our project partner includes two datasets on sale
listings and amenities. StreetEasy collects information from agents by
having them directly enter listing information on the website or through
a feed from their brokerage. StreetEasy verifies the listing and
property information based on records from New York City's Department of
Finance and Department of Buildings {[}3{]}.

Our main data set consists of 59,661 sale listings of 52639 unique
properties listed and/or sold on Streeteasy in 2019. It has 31 variables
including home price, property attributes (e.g., the number of bedrooms,
the number of bathrooms, size in square feet), geospatial information
(e.g.~state, zipcode, longitude, latitude), and a text description of
each listing.

\textbf{(Make a table of variable names and descriptions?)}

\hypertarget{data-preprocessing}{%
\subsection{Data Preprocessing}\label{data-preprocessing}}

\textbf{(More description/plots of EDA here?)}

Exploratory data analyses show that the distributions of the variables
are skewed and 13 of them contain NA values for up to 11242
observations. We performed data cleansing, reduction, and imputation to
pre-process our data before running the analyses. First, we select the
95\% quantile of the non-zero values as a reasonable range of the
listing prices. We also filter out the outliers with unusually large
values for property size and number of bedrooms and bathrooms.

In addition, we use zip codes to fill in an estimate for observations
without valid latitude/longitude information and created more
geographical variables such as city, county, and state using the
\texttt{zipcodeR} package that pulls information from U.S. Census data
{[}4{]}. We perform these two steps because of the varying quality of
geospatial information in the original data set. First, some of the
listings contain four-digit zip codes and erroneous longitude and
latitude values such as (0, 0). Secondly, the original location
information about areas and neighborhoods was entered manually by
agents, so the location variables do not have uniform criteria and may
differ in the level of specificity. Although the new geographic
coordinates might not have street-level precision, by adding a leading
zero to four-digit zip codes and incorporating associated features from
the \texttt{zipcodeR} package, we are able to obtain geographic
information that is consistent across listings.

In order to prevent data leakage for our machine learning models, we
remove the listings that appear more than once in the data set. The
duplicate listings are results of agents posting the same listing
multiple times with updated information. In order to eliminate
duplicates, we first group the listings by \texttt{property\_id} and
keep the ones with the fewest NA values for all variables within each
group. Next, we select the rows with the longest listing descriptions
and the largest number of bedrooms within each group. Last, we use the
\texttt{distinct()} function to keep one unique listing corresponding to
each \texttt{property\_id}.

Because nearly 20\% of the observations do not have values for the
\texttt{size\_sqft} variable, we plan to use the \texttt{mice} package
to impute missing property size values by regression
(\texttt{norm.predict}) {[}5{]}. The predictors of the imputation
consist of the number of bedrooms, the number of bathrooms, unit type,
floor count, city, and state.

\hypertarget{data-analysis-plans}{%
\subsection{Data Analysis Plans}\label{data-analysis-plans}}

In order to determine whether adding text features as predictors will
improve the accuracy of predicting housing price, we first create models
without text features and then compare their model evaluation metrics
with those with extracted text features. We select Root Mean Squared Log
Error (RMSLE) as our evaluation metrics for the linear regression model;
the smaller the RMSLE, the more accurate the model is in predicting the
outcome. To avoid overfitting and ensure the generalizability of our
models, we plan to perform k-fold cross validation on each of our models
and take the mean RMSLE as the metrics for comparison across models.

To establish a baseline, we create a null model that only has mean log
price as the predictor. We expect all other models we build to have
better prediction performances than the null model.

Models without text features. We are working on building a linear
regression model using log price as outcome and a decision tree/ random
forest model that automatically predicts which categories of price range
a listing falls into. For the linear regression model, we plan to use
stepwise selection to select the top 5 most significant predictors of
housing price, build the model using those variables, conduct 5-fold
cross validation, and then compute the mean RMSLE.

\hypertarget{models-with-text-features}{%
\subsection{Models with Text Features}\label{models-with-text-features}}

To convert the listing descriptions into meaningful text-based features,
the words are (1) tokenized: the words inside of the listing
descriptions are split into word or words (i.e., multiple words
tokenzied together are called `n-grams') known as tokens to create a
one-token-per-row for text analysis; (2) vetted for stop words: removing
words that have little to no meaningful information for analysis (e.g.,
`the,' `as,' `a,' `of'); (3) stemmed: finding the base or root of words
(e.g., we want to categorize `windows' the same as `window'), and lastly
(4) analyzed for extracting keywords and n-grams. N-grams generate words
that commonly occur together and improve accuracy of sentiment analysis
models {[}6,7{]}. These steps allow us to infer the value from the
listing descriptions using regular expressions or models. A second
method to featurize the text is the bag-of-words (BoW) approach. This
model enables us to measure the presence of each word throughout all
listing descriptions inside the database. In more technical terms, we
measure the frequency of each token. A supplementary approach to the two
methods indicated above is pre-trained word embeddings or train word
embeddings. To contextualize the semantic relationship of terms, word
embeddings examine the pairwise correlation between words. Assessing
co-occurrences is critical in creating a highly predictive model.

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{0}{0}
\leavevmode\hypertarget{ref-chen2018integrating}{}%
\CSLLeftMargin{1. }
\CSLRightInline{Chen P-H, Zafar H, Galperin-Aizenberg M, Cook T.
Integrating natural language processing and machine learning algorithms
to categorize oncologic response in radiology reports. Journal of
digital imaging. Springer; 2018;31: 178--184. }

\leavevmode\hypertarget{ref-ivanov2020improving}{}%
\CSLLeftMargin{2. }
\CSLRightInline{Ivanov O, Wolf L, Brecher D, Masek K, Lewis E, Liu S, et
al. Improving emergency department ESI acuity assignment using machine
learning and clinical natural language processing {[}Internet{]}. 2020.
Available: \url{http://arxiv.org/abs/2004.05184}}

\leavevmode\hypertarget{ref-streeteasy_2019}{}%
\CSLLeftMargin{3. }
\CSLRightInline{How a listing makes its way to StreetEasy
{[}Internet{]}. StreetEasy. NYC Agent Resource Center; 2019. Available:
\url{https://streeteasy.com/agent-resources/working-with-streeteasy/how-a-listing-makes-its-way-to-streeteasy/}}

\leavevmode\hypertarget{ref-zipcodeR_2020}{}%
\CSLLeftMargin{4. }
\CSLRightInline{Rozzi G. zipcodeR: Data \& functions for working with US
ZIP codes {[}Internet{]}. 2020. Available:
\url{https://CRAN.R-project.org/package=zipcodeR}}

\leavevmode\hypertarget{ref-mice_2011}{}%
\CSLLeftMargin{5. }
\CSLRightInline{van Buuren S, Groothuis-Oudshoorn K. {mice}:
Multivariate imputation by chained equations in r. Journal of
Statistical Software. 2011;45: 1--67. Available:
\url{https://www.jstatsoft.org/v45/i03/}}

\leavevmode\hypertarget{ref-kruczekngrams}{}%
\CSLLeftMargin{6. }
\CSLRightInline{Kruczek J, Kruczek P, Kuta M. Are n-gram categories
helpful in text classification? In: Krzhizhanovskaya VV, Závodszky G,
Lees MH, Dongarra JJ, Sloot PMA, Brissos S, et al., editors.
Computational science -- ICCS 2020. Cham: Springer International
Publishing; 2020. pp. 524--537. }

\leavevmode\hypertarget{ref-pitler2010using}{}%
\CSLLeftMargin{7. }
\CSLRightInline{Pitler E, Bergsma S, Lin D, Church K. Using web-scale
n-grams to improve base NP parsing performance. 2010; }

\end{CSLReferences}

\nolinenumbers


\end{document}
