---
title: "explor_text_analysis_EY"
output: html_document
---
Popular R packages: tidytext, stringr, quanteda, spacyr

Different ways for featurizing text data:
- Define a target feature/tag such as “whether the apartment has been recently renovated” and infer the value from text using regular expressions or models
- Use bag-of-words method by counting the frequency of words observed in each document
- Use pre-trained word embeddings or train word embeddings

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(tidyverse)
library(tidytext)
library(janitor)
library(SnowballC)
```

## Redo bigrams with cleaned data, April 12, 2021
```{r message=FALSE, warning=FALSE}
source("pre-processing.R")
load_data() 
clean_data()
sale_listings_imputed <- impute_data()
```

### Single Word Counts 
```{r fig.height=4, fig.width=6}
sale_text <- sale_listings_imputed %>%
  select(property_id, listing_description)

text <- tibble(txt = sale_text$listing_description)

sale_text_tk <- text %>%
  unnest_tokens(word, txt) %>%
  anti_join(stop_words) 

sale_tk_count <- sale_text_tk %>%
   count(word, sort = TRUE) %>%
  arrange(desc(n))

uni_freq <- sale_tk_count %>%
  head(50) %>%
  ggplot(aes(y=reorder(word,n), x = n)) + 
  geom_bar(stat = "identity", fill="#339999") + 
  labs(title = "Top 50 Frequent Words in Listing Descriptions", x = "count", y="word") +
  theme_minimal()

#stemming 
sale_tk_stem <- text %>%
  unnest_tokens(word, txt) %>%
  anti_join(stop_words) %>%
  mutate(stem = wordStem(word)) 

sale_tk_stem_count <- sale_tk_stem %>%
  count(stem, sort = TRUE) %>%
  arrange(desc(n))

uni_stem_freq <- sale_tk_stem_count %>%
  head(50) %>%
  ggplot(aes(y=reorder(stem,n), x = n)) + 
  geom_bar(stat = "identity")
```

### Bigram Counts 
```{r fig.height=4, fig.width=6}
sale_bigram <- text %>%
  unnest_tokens(bigram, txt, token = 'ngrams', n=2) %>% 
  separate(bigram, c('word1', 'word2'), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  unite(bigram, word1, word2, sep = " ") 

sale_bi_count <- sale_bigram %>%
  count(bigram, sort = TRUE) %>%
  arrange(desc(n))

bi_freq <- sale_bi_count %>%
  head(50) %>%
  ggplot(aes(y=reorder(bigram,n), x = n)) + 
  geom_bar(stat = "identity", fill="#339999") + 
  labs(title = "Top 50 Frequent Bigrams in Listing Descriptions", x = "count", y="bigram") +
  theme_minimal()
bi_freq
```

```{r}
#very raw attempts on linear regression model 
renov_listing <- sale_listings_imputed %>%
  filter(str_detect(listing_description, "renovat*")) %>%
  select(id)

sale_listings_imputed$listing_description <- tolower(sale_listings_imputed$listing_description)

ss_listing <- sale_listings_imputed %>%
  filter(str_detect(listing_description, "stainless steel")) %>%
  select(id)

#text %>%
#  filter(str_detect(txt, "stainless steel"))

sale_listings_tf <- sale_listings_imputed %>%
  mutate(stainless = ifelse(id %in% c(ss_listing$id), 1, 0))

#renovate is not a significant feature in linear regression model 
#rmlse = .52408

#stainless steel - significant; rmlse = .5232
```
```{r}
#tf-idf 
#may not use this 
listing_words_tfidf <- sale_text %>%
  unnest_tokens(word, listing_description) %>%
  anti_join(get_stopwords(), by = "word")  %>%
  count(property_id, word) %>%
  bind_tf_idf(word, property_id, n) 

listing_words_dfm <- listing_words_tfidf %>%
  cast_dfm(property_id, word, tf_idf)

listing_words%>%
  head(100)

#does it even make sense to use tf_idf??? 
listing_words_tfidf %>%
  filter(str_detect(word, "renovat*"))
```
tf_idf would be useful as a search engine but it is not clear how useful it would be to know how many times "renovat*" appear in this document relative to other documents 

## Topic Modeling
```{r}
library(tm)
library(tidytext)
library(tidyverse)
library(SnowballC)
```

```{r}
listing_tokens <- sale_listings_ss %>%
  unnest_tokens(word, listing_description) %>%
  anti_join(stop_words) %>%
  mutate(word = wordStem(word)) 

listing_tokens_n <- sale_listings_ss %>%
  unnest_tokens(word, listing_description) %>%
  anti_join(stop_words) 

#dtm 
#stems 
listing_matrix <- listing_tokens %>%
  count(id, word) %>%
  cast_dtm(document = id, term = word, value = n)

#words with frequency rather than tf_idf 
listing_matrix_n <- listing_tokens_n %>%
  count(id, word) %>%
  cast_dtm(document = id, term = word, value = n) 

#sparse = .80 means 80% of the document contains the word 
sparse_matrix <- removeSparseTerms(listing_matrix, sparse = .80)

sparse_matrix_n <- removeSparseTerms(listing_matrix_n, sparse = .80)


m <- as.matrix(sparse_matrix)

m_n <- as.matrix(sparse_matrix_n)

dtm_data <- as_tibble(m, rownames = "id") %>%
  mutate(id = as.numeric(id))

dtm_data_n <- as_tibble(m_n, rownames = "id")

library(topicmodels)

#alpha > 1: more close to even split prob; alpha < 1 more distinctive 
#delta controls how exclusive a word is assigned to a group 
mod <- LDA(x=listing_matrix_n, k=3, method="Gibbs",control=list(alpha=1, delta=0.1, seed=410))
mod_stem <- LDA(x=listing_matrix, k=3, method="Gibbs",control=list(seed=410))

#beta shows the prob of words in each topic
topics <- tidy(mod, matrix = "beta")
topics_stem <- tidy(mod_stem, matrix = "beta")

#gamma shows the prob of each document in each topic 
listing_topics <- tidy(mod, matrix = "gamma") #%>% 

listing_topics_stem <- tidy(mod_stem, matrix = "gamma")

top_terms <- topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 25) %>% 
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

#terms() select words that have prob > .001 
#terms(mod, threshold = 0.001)


top_terms_stem <- topics_stem %>%
  group_by(topic) %>%
  slice_max(beta, n = 25) %>% 
  ungroup() %>%
  arrange(topic, -beta)

top_terms_stem %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
#topics are not distinct 
```

```{r}
#bigram topics 
listing_bigram <- sale_listings_ss %>%
  unnest_tokens(bigram, listing_description, token = 'ngrams', n=2) %>%
  separate(bigram, c('word1', 'word2'), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  unite(bigram, word1, word2, sep = " ") 

listing_matrix_bi <- listing_bigram %>%
  count(id, bigram) %>%
  cast_dtm(document = id, term = bigram, value = n)


listing_matrix_bi_n <- listing_bigram %>%
  count(id, bigram)%>% 
  group_by(bigram) %>% 
   # Filter for corpus wide frequency
   filter(sum(n) >= 10) %>% 
   # Ungroup the data andreate a document term matrix
   ungroup() %>% 
  cast_dtm(document = id, term = bigram, value = n)

sparse_matrix_bi <- removeSparseTerms(listing_matrix_bi, sparse = .80)

m_bi <- as.matrix(sparse_matrix)

dtm_data_bi <- as_tibble(m, rownames = "id") %>%
  mutate(id = as.numeric(id))

mod_bi <- LDA(x=listing_matrix_bi, k=3, method="Gibbs",control=list(seed=410))

topics_bi <- tidy(mod_bi, matrix = "beta")
listing_topics_bi <- tidy(mod_bi, matrix = "gamma")

top_terms_bi <- topics_bi %>%
  group_by(topic) %>%
  slice_max(beta, n = 25) %>% 
  ungroup() %>%
  arrange(topic, -beta)

top_terms_bi %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
#topics doesn't make sense 
```






