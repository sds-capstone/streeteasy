---
title: "gradient boosting"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(tidyverse)
library(tidymodels)
library(leaps)
library(knitr)
library(MLmetrics)
library(caret)
library(randomForest)
library(ie2misc)
library(Ecdat)
library(xgboost)
library(formattable)
```

```{r message=FALSE, warning=FALSE}
#use imputed data for linear model 
source("pre-processing.R")
load_data() # creates two dataframes in the global environment

clean_data() # cleans the data

sale_listings_imputed <- impute_data() # imputes size values and returns a new dataframe
```

### More data cleaning from Yanwan on unittype and cities 
```{r}
sale_listings_imputed <- sale_listings_imputed %>%
    mutate(unit_group = ifelse(unittype %in% c("B", "M", "Z"), 1, 
                                ifelse(unittype %in% c("H", "T"), 2, 
                                       ifelse(unittype %in% c("A", "C", "R", "P", "Y"), 3, 
                                              ifelse(unittype %in% c("D", "F", "N"), 4,
                                                     ifelse(unittype %in% c("X", "U", "?"), 5, unittype)))))) %>%
    mutate(unit_group = as.factor(unit_group))
  
sale_listings_imputed <- sale_listings_imputed %>%
    mutate(city_group = ifelse(major_city == "Long Island City", 1, 
                               ifelse(major_city == "New York", 2, 
                                      ifelse(major_city %in% c("Astoria", "Brooklyn",
                                                               "Hoboken", "Toms River",
                                                               "Flushing", "Edgewater",
                                                               "Elmhurst", "Sunnyside"), 3,
                                             4)))) %>%
    mutate(city_group = as.factor(city_group))

sale_listings_imputed <- sale_listings_imputed %>%
  mutate(is_historic = ifelse(is.na(is_historic) == TRUE, "0", is_historic))

sale_listings_ss <- sale_listings_imputed %>%
  mutate(listing_description = ifelse(is.na(listing_description) == TRUE, 0, listing_description)) %>%
  mutate(
    stainless_steel = as.integer(str_detect(listing_description, "[Ss]tainless.[Ss]teel")),
    hw_floors = as.integer(str_detect(listing_description, "[Hh]ardwood.[Ff]loors?")),
    wd = as.integer(str_detect(listing_description, c(
      "[Ww]d",
      "[Ww]/d",
      "[Ww]&d",
      "[Ww] & d",
      "[Ww]asher/dryer",
      "[Ww]asher / dryer",
      "[Ww]asher and dryer",
      "[Ww]asher&dryer",
      "[Ww]asher & dryer",
      "[Ww]asher-dryer",
      "[Ww]asher dryer"
    ))),
    steel_app = as.integer(str_detect(listing_description, "[Ss]teel.[Aa]ppliances?")),
    fitness = as.integer(str_detect(listing_description, c(
      "[Ff]itness",
      "[Ff]itness center",
      "[Ff]itness-center"
    ))),
    
    # top 10 words with highest price avergae and highest frequency
    marble = as.integer(str_detect(listing_description, "[Mm]arble")),
    master = as.integer(str_detect(listing_description, "[Mm]aster")),
    views = as.integer(str_detect(listing_description, "[Vv]iews")),
    custom = as.integer(str_detect(listing_description, "[Cc]ustom")),
    floor = as.integer(str_detect(listing_description, "[Ff]loor")),
    private = as.integer(str_detect(listing_description, "[Pp]rivate")),
    windows = as.integer(str_detect(listing_description, "[Ww]indows")),
    dining = as.integer(str_detect(listing_description, "[Dd]ining")),
    offers = as.integer(str_detect(listing_description, "[Oo]ffers")),
    light = as.integer(str_detect(listing_description, "[Ll]ight")),
    #----------------------------------------------------------------
    renovate = as.integer(str_detect(listing_description, "[Rr]enovat*")),
    closet_space = as.integer(str_detect(listing_description, "[Cc]loset.[Ss]pace")),
    spacious = as.integer(str_detect(listing_description, "[Ss]pacious*")),
    storage = as.integer(str_detect(listing_description, "[Ss]torage")),
    closet_space = as.integer(str_detect(listing_description, "[Cc]loset.[Ss]pace")),
    roof_deck = as.integer(str_detect(listing_description, "[Rr]oof.[Dd]eck")),
    park = as.integer(str_detect(listing_description, "[Pp]ark")),
    balcony = as.integer(str_detect(listing_description, "[Bb]alcony")),
    courtyard = as.integer(str_detect(listing_description, "[Cc]ourtyard")),
    view = as.integer(str_detect(listing_description, "[Vv]iew*")),
    window = as.integer(str_detect(listing_description, "[Ww]indow*")),
    natural_light = as.integer(str_detect(listing_description, "[Nn]atural.[Ll]ight")),
    en_suite = as.integer(str_detect(listing_description, "[Ee]n.[Ss]uite")),
    pet_friendly = as.integer(str_detect(listing_description, "[Pp]et.[Ff]riendly")),
    tree_lined = as.integer(str_detect(listing_description, "[Tt]ree.[Ll]ined")),
    central_park = as.integer(str_detect(listing_description, "[Cc]entral.[Pp]ark")),
    outdoor_space = as.integer(ifelse((park | balcony | courtyard | roof_deck | central_park | tree_lined) == 1, 1, 0))                              
  )


```

```{r split data to train and test}
#error: cannot change value of locked binding for 'train'
#ml_setup(sale_listings_imputed) 

train <- sale_listings_ss%>% sample_frac(.60)
test <- sale_listings_ss %>% anti_join(train)

#+1, -1 are supposed to optimize rmlse
train <- train %>%
  mutate(log10price = log10(price + 1)) %>%
  mutate(log10size = log10(size_sqft))

test <- test %>%
  mutate(log10size = log10(size_sqft))


```


## Model setup 


```{r}
#no text features 
#year_built has lots of NA 
model_formula <-
  "log10price ~ bedrooms + 
                bathrooms + 
                log10size + 
                floor_count + 
                is_historic + 
                state + 
                unit_group + 
                city_group " %>%
  as.formula()
```



```{r}
#with some text features 
#year_built has lots of NA 
model_formula_txt <-
  "log10price ~ bedrooms + 
                bathrooms + 
                log10size + 
                floor_count + 
                is_historic + 
                state + 
                unit_group + 
                city_group +
                stainless_steel +
                hw_floors +
                renovate + 
                outdoor_space + 
                central_park +
                marble +
                master +
                views +
                custom +
                floor +
                private +
                windows +
                dining +
                offers +
                light " %>%
  as.formula()
```

<!-- http://www.sthda.com/english/articles/35-statistical-machine-learning-essentials/139-gradient-boosting-essentials-in-r-using-xgboost/ --> 

```{r}
fit_control <- trainControl(method = "cv", number = 10)

# parameters explained below 
grid<-expand.grid(nrounds = 150, 
                  max_depth = 3, 
                  eta = 0.4, 
                  gamma = 0, 
                  colsample_bytree = 0.8 , 
                  min_child_weight = 1,
                  subsample = 1)
```

<!-- What does each parameter mean? 

nrounds: the number of decision trees in the final model

max_depth: Controls the maximum depth of the trees. Deeper trees have more terminal nodes and fit more data. Convergence also requires less trees if we grow them deep. However, if the trees are to deep we will be using a lot of information from the first trees and the final trees of the algorithm will have less importance on the loss function. The Boosting benefits from using information from many trees. Therefore it is intuitive that huge trees are not desirable. Smaller trees also grow faster and because the Boosting grow new trees in a pseudo-residual and we do not require any amazing adjustment for an individual tree.

eta: Prevents overfitting. Learning (or shrinkage) parameter. It controls how much information from a new tree will be used in the Boosting. This parameter must be bigger than 0 and limited to 1. If it is close to zero we will use only a small piece of information from each new tree. If we set eta to 1 we will use all information from the new tree. Big values of eta result in a faster convergence and more over-fitting problems. Small values may need to many trees to converge.

gamma: Controls the minimum reduction in the loss function required to grow a new node in a tree. This parameter is sensitive to the scale of the loss function, which will be linked to the scale of your response variable. The main consequence of using a gamma different from 0 is to stop the algorithm from growing useless trees that barely reduce the in-sample error and are likely to result in over-fitting. I will leave this parameter to a future post to save some space here.

colsample_bytree: This is a family of parameters for subsampling of columns. Too much to write, look here https://xgboost.readthedocs.io/en/latest/parameter.html

sub_sample: This parameter determines if we are estimating a Boosting or a Stochastic Boosting. If we use 1 we obtain the regular Boosting. Values between 0 and 1 are for the stochastic case. The stochastic Boosting uses only a fraction of the data to grow each tree. For example, if we use 0.5 each tree will sample 50% of the data to grow. Stochastic Boosting is very useful if we have outliers because it limits their influence on the final model because they are dropped on several sub-samples. Moreover, it is possible to have a significant improvement on smaller instances because they are more susceptible to over-fitting.

min_child_weigth: Controls the minimum number of observations (instances) in a terminal node. The minimum value for this parameter is 1, which allows the tree to have terminal nodes with only one observation. If we use bigger values we limit a possible perfect fit on some observations. 

--> 
## Model fitting (the caret models take super long to load)
```{r}
#no text features 
model_rf <- randomForest(
  # Model formula
  form = model_formula,
  # Training data
  data = train, 
  # At each node of tree, number of features/predictor variables to randomly
  # choose from for splitting:
  mtry = 3,
  # Number of bagged (bootstrap aggregated) trees in your forest:
  ntree = 150 
)

print(model_rf)
```

```{r}
#use caret
model_rf_caret <- caret::train(
  # Model formula
  form = model_formula,
  # Training data
  data = train, 
  # Set method to randomForests. Note: this is where you can switch out to
  # different methods
  method = "xgbTree",
  # Score/error metric used:
  metric = "RMSE",
  # Cross-validation settings:
  trControl = fit_control,
  # Search grid of tuning parameters
  tuneGrid = grid,
  na.action = na.pass
)
```

```{r}
print(model_rf_caret)
```

```{r}
#with text features 
model_rf_txt <- randomForest(
  # Model formula
  form = model_formula_txt,
  # Training data
  data = train, 
  # At each node of tree, number of features/predictor variables to randomly
  # choose from for splitting:
  mtry = 3,
  # Number of bagged (bootstrap aggregated) trees in your forest:
  ntree = 150
)
```

```{r}
print(model_rf_txt)
```



```{r}
#use caret
model_rf_caret_txt <- caret::train(
  # Model formula
  form = model_formula_txt,
  # Training data
  data = train, 
  # Set method to randomForests. Note: this is where you can switch out to
  # different methods
  method = "xgbTree",
  # Score/error metric used:
  metric = "RMSE",
  # Cross-validation settings:
  trControl = fit_control_txt,
  # Search grid of tuning parameters
#  tuneGrid = grid,
  na.action = na.pass
  )
```

```{r}
print(model_rf_caret_txt)
```

## Predict on test 

  <!-- No Text --> 
```{r}
#no text features 
test_rf <- test %>%
  mutate(
    log10price_hat = predict(model_rf, test),
    price_hat = 10^log10price_hat - 1
  )

test_rf_caret <- test %>%
  mutate(
    log10price_hat = predict(model_rf_caret, test),
    price_hat = 10^log10price_hat - 1
  )
```

  <!-- Text -->  
```{r}
test_rf_txt <- test %>%
  mutate(
    log10price_hat = predict(model_rf_txt, test),
    price_hat = 10^log10price_hat - 1
  )
test_rf_caret_txt <- test %>%
  mutate(
    log10price_hat = predict(model_rf_caret_txt, test),
    price_hat = 10^log10price_hat - 1
  )
```



```{r}
# RMSLE
rmsle_text <- RMSLE(test_rf_txt$price_hat, test_rf_txt$price)
rmsle_text_caret <- RMSLE(test_rf_caret_txt$price_hat, test_rf_txt$price)
rmsle_notext <- RMSLE(test_rf$price_hat, test$price)
rmsle_notext_caret <- RMSLE(test_rf_caret$price_hat, test$price)

# RMSE
rmse_text <- MLmetrics::RMSE(test_rf_txt$price_hat, test_rf_txt$price)
rmse_text_caret <- MLmetrics::RMSE(test_rf_caret_txt$price_hat, test_rf_txt$price)
rmse_notext <- MLmetrics::RMSE(test_rf$price_hat, test$price)
rmse_notext_caret <- MLmetrics::RMSE(test_rf_caret$price_hat, test$price)

# MAPE
mape_text <- mape_vec(test_rf_txt$price_hat, test_rf_txt$price, na_rm = TRUE)
mape_text_caret <- mape_vec(test_rf_caret_txt$price_hat, test_rf_txt$price, na_rm = TRUE)
mape_notext <- mape_vec(test_rf$price_hat, test$price, na_rm = TRUE)
mape_notext_caret <- mape_vec(test_rf_caret$price_hat, test$price, test$price, na_rm = TRUE)

```


```{r}
# Table 
table <- data.frame(Type = c("text", 
                              "text_caret", 
                              "without text", 
                              "without text_caret"), 
                    
                    RMSLE = c(rmsle_text, 
                              rmsle_text_caret, 
                              rmsle_notext, 
                              rmsle_notext_caret),
                    
                    RMSE = c(rmse_text, 
                             rmse_text_caret, 
                             rmse_notext, 
                             rmse_notext_caret),
                    
                    MAPE = c(mape_text, 
                             mape_text_caret, 
                             mape_notext, 
                             mape_notext_caret))
kable(table)
```

<!-- Create nicer table
```{r}
formattable(table, 
            align = "l", 
            digits = 5,
            list(`Type` = formatter("span", style = ~ style(color = "black")),
                 `RMSLE` = formatter("span", style = ~ style(color = "black")),
                 `RMSE` = formatter("span", style = ~ style(color = "black")) 
                 ))
```
--> 



