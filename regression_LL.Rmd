---
title: "regression_LL"
output: html_document
---

```{r loading packages}
library(dplyr)
library(stringr)
library(rpart)
library(rattle)
library(ggplot2)
library(tidymodels)
library(rpart.plot)
```

```{r loading files and joinging dataframes}
amenities <- read.csv("/Users/llow/Desktop/capstone/streeteasy_data/amenities.csv")
listings <- read.csv("/Users/llow/Desktop/capstone/streeteasy_data/sale_listings.csv")
joined <- full_join(amenities, listings, by = "property_id")
```

```{r cleaning data}
joined$size_sqft[joined$size_sqft == 0] <- NA
joined
listings$size_sqft[listings$size_sqft == 0] <- NA
listings
```


```{r comparing price to other variables to see if there is a correlation}
pairs_plot <- pairs(~bedrooms + bathrooms + size_sqft + price, data = listings,
   main = "Simple Scatterplot Matrix",
   upper.panel = NULL)
```

```{r creating and plotting linear regression}
reg <- lm(price ~ bedrooms + bathrooms + size_sqft, data = listings)
summary(reg)
plot(reg)
```


1. shows that data may be better modeled by a parabola not a line
2. distribution is not normal and has lots of data on the tails
3. there is not equal variance between points
4. there are three major outliers that are screwing up this particular model
conclusion: this data should not be modeled linearly 

```{r lets try adding more variables}
reg_extra <- lm(price ~ bedrooms + bathrooms + size_sqft + year_built + floor_count + time_to_subway + half_baths, data = listings)
summary(reg_extra)
plot(reg_extra)
```
definitely shouldn't be modeled linearly...

```{r}
quad_reg <- lm(price ~ poly(bedrooms, 3) + poly(bathrooms, 2) + poly(floor_count, 2), data = listings)
summary(quad_reg)
plot(quad_reg)
```

```{r creating and plotting linear regression}
reg_test <- lm(price ~ bedrooms + area_name, data = listings)
summary(reg_test)
plot(reg_test)
```

looks like linear and polynomial regression is not the way to go.  let's try decision tree analysis:

```{r simple decision tree}
train <- c(sample(1:50, 25), sample(51:100, 25), sample(101:150, 25))
tree_num <- rpart(price ~ bedrooms + bathrooms + size_sqft + time_to_subway + year_built, data = listings, subset <- train, method = "class")
fancyRpartPlot(tree)
```

```{r prediction analysis}
tree_cat <- rpart(price ~ addr_city, data = listings, subset = train, method = "class")
fancyRpartPlot(tree_c)
```

```{r visualizing location data}
plot(listings$addr_lat, listings$price)
plot(listings$addr_lon, listings$price)

ggplot(listings, aes(x = addr_lon, y = addr_lat)) + 
  geom_point() + 
  geom_text(label = rownames(listings))
```

seems like most houses are around 41 deg, -75 deg.  Are the lat, lon around 0 false entries? let's delete the ones that seem fishy and see what happens.

```{r narrowing location data}
listings_adj <- listings %>%
  filter(addr_lat > 35,
         addr_lon < -70, 
         addr_lon > -76)
```

```{r revisualizing location data}
plot(listings_adj$addr_lat, listings_adj$price)
plot(listings_adj$addr_lon, listings_adj$price)

ggplot(listings_adj, aes(x = addr_lon, y = addr_lat)) + 
  geom_point() 
```


```{r using location data to predict price}
latlontree <- rpart(price ~ addr_lat + addr_lon, data = listings_adj)
fancyRpartPlot(latlontree)
```

```{r simple decision tree with location and other variables}
tree_all = rpart(price ~ bedrooms + bathrooms + size_sqft + time_to_subway + year_built + addr_lat + addr_lon, data = listings_adj, subset = train, method = "class")
fancyRpartPlot(tree)
```

```{r}
price_prediction <- predict(tree_all, listings, type = 'class')

# comparing predictions to actual prices
table <- table(Price = listings$price, Prediction = price_prediction)

accuracy_Test <- sum(diag(table)) / sum(table)

print(paste('Accuracy for test', accuracy_Test))
```
oh no, this is a really really low accuracy...
(source: https://www.guru99.com/r-decision-trees.html#6)

