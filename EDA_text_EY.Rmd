---
title: "EDA_text_EY"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(tidyverse)
library(tidytext)
library(janitor)
library(SnowballC)
```

```{r message=FALSE, warning=FALSE}
library(readr)
sale_listings <- read_csv("data/sale_listings.csv") 
amenities <- read_csv("data/amenities.csv")
```

## Data Cleaning 
```{r}
#make state names consistent 
sale_listings_clean <- sale_listings %>%
  mutate(addr_state = ifelse(addr_state == "New Jersey", "NJ", ifelse(addr_state == "New York", "NY", addr_state)))

#create log price var 
sale_listings_clean <- sale_listings_clean %>%
  mutate(log10_price = log10(price))

```


```{r message=FALSE, warning=FALSE, paged.print=TRUE}
#make the spelling of cities consistent 
sale_listings_clean <- sale_listings_clean %>%
  mutate(addr_city = tolower(addr_city)) %>%
  mutate(addr_city = recode(addr_city,
                            "apt.7" = "west new york",
                            "1214 - north brunswick" = "north bergen",
                            "77 hudson - jersey city" = "jersey city",
                            "arvene" = "arverne",
                            "bayonne city" = "bayonne",
                            "brook;yn" = "brooklyn",
                            "brookyn" = "brooklyn",
                            "e. elmhurst" = "east elmhurst",
                            "jersey city bergen/ lafayette" = "jersey city bergen-lafayett",
                            "jc, bergen-lafayett" = "jersey city bergen-lafayett",
                            "jc, downtown" = "jersey city downtown",
                            "jc, greenville" = "jersey city greenville",
                            "jc, journal square" = "jersey city jsq",
                            "journal square" = "jersey city jsq",
                            "kearney" = "kearny",
                            "neponsit ny" = "neponsit",
                            "queen" = "queens",
                            "queens village n" = "queens village",
                            "richmond hill s." = "richmond hill",
                            "west ny" = "west new york",
                            "springfield gdns" = "springfield gardens",
                            "s. ozone park" = "south ozone park"))

sale_listings_clean %>%
  group_by(addr_city) %>%
  summarize(mean = mean(price), n=n()) 

amenities %>%
  group_by(name) %>%
  summarize(n=n())


sale_listings_clean %>%
  group_by(area_name) %>%
  summarize(mean = mean(price), n=n()) 

sale_listings_clean %>%
  group_by(addr_hood) %>%
  summarize(mean = mean(price), n=n()) 
```
- there are 211 unique zip codes - do we want to use the addr_zip as a predictor? It seems like on the website of Zillows group, zip codes are included to provide estimated price 

- If we want to use addr_city as a predictor, we need to decide on what level of municipalities we want to use. Right now there are 113 unique addr_city after cleaning. Some listed "cities" are actually neighborhoods in the county and only has one listing. How do we want to deal with that? 

-Use geospatial coordinates; use zip code 

## Exploratory Data Analysis 
```{r}
#the unique property id of amenities and sale_listings do not match
length(unique(amenities$property_id))
#
length(unique(sale_listings$property_id))

summary(sale_listings_clean)

#examine listings with odd numbers 

sale_listings_clean %>%
  filter(bedrooms == 99)

#sale_listings_clean %>%
#  arrange(desc(bedrooms))

#sale_listings_clean %>%
#  arrange(desc(bathrooms))

#sale_listings_clean %>%
#  filter(price == 1)

#sale_listings_clean %>%
#  filter(size_sqft == 588527)

#sale_listings_clean %>%
#  filter(year_built == 0)

unique(sale_listings_clean$bedrooms)

sale_listings_clean %>%
  filter(bedrooms == 44)
```

### Questions: 
- the maximum number is 99 for bedrooms, 66 for bathrooms, 3200 for anyrooms, 247 for building_count (n. of buildings). I tend to think the listing with 99 bedrooms is a data entry mistake since it only has 9 rooms in total? The two listings with 66 bathrooms also seem impossible since it does not have data on size_sqft. Also the maximum for size_sqft is 588527, which seems odd. 

A methodological question: how could we systematically identify data that seems odd and how do we know if that's a mistake in data entry or not 

- variables missing over 6000 obs: time_to_subway (10274 NA), census_block (12011 NA), building_count (11242 NA), residential_unit_count (10635 NA), total_unit_count (11878 NA), year_built (7839 NA)

- the minimum price is 1, which doesn't make sense; it corresponds to two listings with the same addr_stree, longitude & latitude, but different size_sqft. There are 315 entries with year built = 0. 

- although anyrooms mean the total number of rooms, it is strange that sometimes anyrooms is smaller than/ equal to bedrooms

- In data dictionary, it says "One property can correspond to more than listing, if it has been listed more than once." Should we remove duplicates based on some criteria (if there is any)? 


## Data visualization 

```{r message=FALSE, warning=FALSE}

#histogram of price: extremely skewed 
mean_price <- mean(sale_listings_clean$price)
mean_price_p <- sale_listings_clean %>%
  ggplot(aes(x=price)) +
  geom_histogram() +
  geom_vline(xintercept = mean_price, size = 1, color = "blue")

#log price - looks better 
sale_listings_clean %>%
  ggplot(aes(x=price)) +
  geom_histogram(fill = "#0c4c8a") +
  scale_x_log10() +
  geom_vline(xintercept = mean_price, size = 1, color = "yellow") +
  ggtitle("mean log price") + 
  theme_minimal()

mean_st <- sale_listings_clean %>%
  group_by(addr_state) %>%
  summarize(n = n(), mean = mean(price))

#compare log distribution and mean price between NJ and NY
ggplot(data=sale_listings_clean, aes(x=price, fill=addr_state)) +
  geom_density(adjust=1.5, alpha=.4) +
  geom_vline(data = mean_st, aes(xintercept = mean, color = addr_state), linetype="dashed") +
  scale_x_log10() +
  ggtitle("mean log price by state")
```

Mean price = 1527538. NJ has 8877 listings, raw mean price is 666404.3; NY has 50784 listings, mean price is 1678063.1.  

```{r message=FALSE, warning=FALSE}
#correlation plot 
sale_cor <- sale_listings_clean %>%
  dplyr::select(bedrooms, bathrooms, anyrooms, size_sqft, time_to_subway, floor_count, year_built, log10_price)

sale_cor %>%
  GGally::ggpairs()
```

- many variables are extremely skewed: how do we deal with that 

## tokenization, bigrams, and word counts
```{r fig.height=4, fig.width=6}
#only select property_id and descriptions 
sale_text <- sale_listings %>%
  select(property_id, listing_description)

text <- tibble(txt = sale_text$listing_description)

#split text into single words and remove stop words 
sale_text_tk <- text %>%
  unnest_tokens(word, txt) %>%
  anti_join(stop_words) 

sale_tk_count <- sale_text_tk %>%
   count(word, sort = TRUE) %>%
  arrange(desc(n))

uni_freq <- sale_tk_count %>%
  head(50) %>%
  ggplot(aes(y=reorder(word,n), x = n)) + 
  geom_bar(stat = "identity")

uni_freq + theme(axis.text.x=element_text(angle=45, hjust=1))
```

```{r fig.height=4, fig.width=6, message=FALSE, warning=FALSE}
#split text into bigrams and remove stop words 
#2 grams take forever to run 
sale_bigram <- text %>%
  unnest_tokens(bigram, txt, token = 'ngrams', n=2) %>% 
  separate(bigram, c('word1', 'word2'), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  unite(bigram, word1, word2, sep = " ") 

sale_bi_count <- sale_bigram %>%
  count(bigram, sort = TRUE) %>%
  arrange(desc(n))


bi_freq <- sale_bi_count %>%
  head(50) %>%
  ggplot(aes(y=reorder(bigram,n), x = n)) + 
  geom_bar(stat = "identity")
bi_freq
```

## Data Cleaning continue: March 17, 2021 

```{r}
#examine percentile of the data 
#keep looking at price = 1; and it seems like 95% is good place to go 
quantile(sale_listings$price, probs = c(0, 0.125,0.375, 0.5, 0.625, 0.875, 0.9, 0.95, 0.99, 1))
```

```{r}
#examine housing price by housing type 
#C - commercial has the highest avg_price, then townhouse 
#if we were to use housing types as a var, make sure to filter out type X since it only has one entry 
sale_listings %>%
  group_by(unittype) %>%
  summarize(n=n(), avg_price = mean(price)) %>%
  arrange(desc(avg_price))
```

```{r}
#original listings: 
#https://www.zillow.com/homes/160-Carroll-St-Apt-1-Brooklyn,-NY,-11231_rb/2113383776_zpid/
#https://www.zillow.com/homedetails/160-Carroll-St-APT-2-Brooklyn-NY-11231/2115094736_zpid/

#https://streeteasy.com/property/9776851-160-carroll-street-1
# id9776851: $1,925,000 on 10/30/2020
#https://streeteasy.com/property/7854659-160-carroll-street-2
#id 7854659: $2,275,000 on 09/29/20 
sale_listings %>%
  filter(price == 1)
#so they are NOT duplicates but in the same floor same building, and their price is not 1 -- in this case should we N/A their price? Use website info to fill in? 

#after removing the lowest price, the lowest price is 40000
sale_listings_1 <- sale_listings %>%
  filter(price!=1) 

quantile(sale_listings_1$price, probs = c(0, 0.125,0.375, 0.5, 0.625, 0.875, 0.9, 0.95, 0.99, 1))
#df %>% filter(Point < quantile(df$Point, 0.95))

#smaller or equal to 95 percentile 
sale_listings_1 <- sale_listings_1 %>%
  filter(price <= quantile(sale_listings_1$price, 0.95))
```

```{r}
#still many wierd entries  
summary(sale_listings_1)

quantile(sale_listings_1$bedrooms, probs = c(0, 0.125,0.375, 0.5, 0.625, 0.875, 0.9, 0.95, 0.99, 1))

#https://streeteasy.com/property/7954592-1150-5-avenue-1a -- still 99 beds 
sale_listings_1 %>%
  filter(bedrooms == 99)
#1150 Fifth Avenue is a building 
sale_listings_1 %>%
  filter(addr_street == "1150 Fifth Avenue")
#didn't not find unit 1A that has 99 bedrooms online 

sale_listings_1 %>%
  filter(addr_street == '151 COLUMBIA AVENUE')
#https://www.zillow.com/homedetails/151-Columbia-Ave-Jersey-City-NJ-07307/68301038_zpid/
#based on the link I think bathrooms should be 6, but there is no sqrt_feet, sold price is 999,900 
#do we want to pull the price from the website? we don't know when the data is pulled and the price online might be different 
sale_listings_1 %>%
  filter(bathrooms == 66)


#wow so 12.5% of the data has 0 sqrt_feet 
quantile(sale_listings_1$size_sqft, probs = c(0, 0.125,0.375, 0.5, 0.625, 0.875, 0.9, 0.95, 0.99, 1), na.rm = TRUE)

#size_sqrt has 10959 missing values and 10222 0 values 

# https://streeteasy.com/building/226-west-111-street-new_york#tab_building_detail=2
#for 226 West 111th St unit 11, the website on streeteasy shows 0 sqrt_ft for listings
#https://www.zillow.com/homedetails/226-W-111th-St-APT-11-New-York-NY-10026/2122774251_zpid/
#but when search on zillow, the size is available, and the price matches 

#https://streeteasy.com/property/6831651-401-east-65-street-4b
#property id 6831651, 550 ft 

#n. of 0 values 
sum(sale_listings_1$size_sqft == 0, na.rm = TRUE)

#sale_listings_1 %>%
#  filter(size_sqft == 0)

sale_listings_2 <- sale_listings_1 %>%
  filter(size_sqft != 588527) %>%
  filter(bedrooms != 99) %>%
  filter(bathrooms != 66) 

#21 rows for time_to_subway == 6200617 
sale_listings_1 %>%
  filter(time_to_subway == 6200617)

#dont understand where the time_to_subway is 
#https://streeteasy.com/building/114-butler-street-brooklyn
#property is 9690755: where to check time_to_subway? 
sale_listings_1 %>%
  filter(addr_street == "114 Butler Street")
```

```{r}
summary(sale_listings_1)
quantile(sale_listings$anyrooms, probs = c(0, 0.125,0.375, 0.5, 0.625, 0.875, 0.9, 0.95, 0.99, 1), na.rm = TRUE)
#https://streeteasy.com/building/316-dongan-hills-avenue-staten_island
#how's anyroom defined? 
#make anyroom == 3200 NA 
#question: for unusual numbers in one var of a listing, do we NA the var or delete the entry?
sale_listings_3 <- sale_listings_2 %>%
  mutate(time_to_subway = ifelse(time_to_subway %in% c(6200617,74574), NA, time_to_subway))

summary(sale_listings_3)

#how to know if anyrooms is reasonable or not?? 
sale_listings %>%
  arrange(desc(anyrooms)) %>%
  head(50)

quantile(sale_listings$time_to_subway, probs = c(0, 0.125,0.375, 0.5, 0.625, 0.875, 0.9, 0.95, 0.99, 0.995, 0.999, 1), na.rm = TRUE)

##variables that require big brains to clean: anyrooms, time_to_subway 
```


### Geospatial Data 
```{r}
summary(sale_listings)
```

```{r}
#zipcodeR is a quite new package; there are other ways to gather lat/lon such as using tidygeocoder or ggmap. The lon/lat i created is based on zipcode and does not have the precision of using street names 
library(zipcodeR)
data("zip_code_db")
#dataset from zipcodeR 
zip_code <- zip_code_db %>%
  select(zipcode, major_city, county, state, lat, lng)

#some zipcodes do not have leading zeros - they should be NJ zipcodes 
sale_listings_join <- sale_listings %>%
  mutate(addr_zip = ifelse(addr_zip < 10000, paste0("0", addr_zip), addr_zip)) %>%
  rename("zipcode" = "addr_zip") %>%
  mutate(zipcode = as.character(zipcode))

sale_listings_join <- left_join(sale_listings_join, zip_code, by = "zipcode")

#filter those who have zipcode, but do not have lon/lat in zipcodeR 
sale_listings_join %>%
  filter(is.na(zipcode) == FALSE, is.na(lat) == TRUE) %>%
  group_by(zipcode) %>%
  summarize(n=n())

#since the lat/lon in sale_listings have some unusual coordinates, i mainly use coordinates in zip_code_db since they are really similar
sale_listings_join_1 <- sale_listings_join %>%
  mutate(lat = ifelse(zipcode %in% c("11249", "11466"), addr_lat, lat),
         lng = ifelse(zipcode %in% c("11249", "11466"), addr_lon, lng)) 

sale_listings_join_1 %>%
  filter(is.na(listing_description) ==TRUE)

```


## Remove duplicates 0324
```{r}
#sale_listings 59661
#unique property_id = 51355
length(unique(sale_listings$property_id))
length(unique(sale_listings$id))

#my_data %>% distinct(Sepal.Length, .keep_all = TRUE)

#only keep the vars we want to use 
sale_listings_join_c <- sale_listings_join_1 %>%
  select(id, property_id, unittype, listing_description, bedrooms, bathrooms, size_sqft, price, addr_street, addr_unit, zipcode, floor_count, year_built, is_historic, major_city:lng)

#5763 property_id has duplicates 
duplicate_id <- sale_listings_join_c %>%
  filter(!is.na(property_id)) %>%
  group_by(property_id) %>%
  summarize(n=n()) %>%
  filter(n > 1)

#make 0 size_sqft 0 
#1284 NA in property id 
#sale_listings %>%
#  filter(is.na(property_id) == TRUE)
#12,786 listings should eventually only keep 5763
duplicate_listing <- sale_listings_join_c %>%
  mutate(size_sqft = ifelse(size_sqft == 0, NA, size_sqft)) %>%
  filter(property_id %in% duplicate_id$property_id) %>%
  arrange(property_id)

#choose the larger id for same property_id?? 
#property_id == 77131, the smaller id is the more recent one 

#count row NA 
na_row <- as.data.frame(rowSums(is.na(duplicate_listing))) %>%
  rename("na_row" = "rowSums(is.na(duplicate_listing))")

duplicate_na <- cbind(duplicate_listing, na_row)



#choose listing that has less NA attributes 
#11,334 listings 
duplicate_listing_2 <- duplicate_na %>%
  group_by(property_id) %>%
  filter(na_row == min(na_row)) 

library(stringr)

length_des <- as.data.frame(nchar(duplicate_listing_2$listing_description)) %>%
  rename("len_des" = "nchar(duplicate_listing_2$listing_description)") %>%
  mutate(len_des = as.numeric(len_des))

duplicate_des <- cbind.data.frame(duplicate_listing_2, length_des)

#keep the one with longer description
final_clean <- duplicate_des  %>%
  mutate(len_des = ifelse(is.na(len_des) == TRUE, 0, len_des)) %>%
  group_by(property_id) %>%
  filter(len_des == max(len_des)) %>%
  filter(bedrooms == max(bedrooms)) %>%
  arrange(property_id) %>%
  distinct(property_id, .keep_all = TRUE)

```





Citation: 
https://www.gavinrozzi.com/post/an-r-package-for-zip-codes/

https://www.datanovia.com/en/lessons/identify-and-remove-duplicate-data-in-r/


