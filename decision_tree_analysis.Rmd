---
title: "decision_tree_analysis"
output: html_document
---

```{r loading packages, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(rattle)
library(rpart)
library(randomForest)
library(MLmetrics)
library(textdata)
library(tidytext)
library(SnowballC)
```

```{r message=FALSE, warning=FALSE}
#use imputed data for linear model 
source("pre-processing.R")
load_data() # creates two dataframes in the global environment

clean_data() # cleans the data

sale_listings_imputed <- impute_data() # imputes size values and returns a new dataframe

#call data with all text-based variables 
source("text_processing.R")

text_var()
```

```{r}
ml_setup(sale_listings_imputed) 

#+1, -1 are supposed to optimize rmlse, but it seems like the result is the same at this point 
train <- train %>%
  mutate(log10price = log10(price + 1)) %>%
  mutate(log10size = log10(size_sqft))

test <- test %>%
  mutate(log10size = log10(size_sqft))
```

This is old decision tree analysis:

```{r narrowing location data}
# we should group into city blocks or neighborhoods, maybe in pre-processing?
# sale_listings_adj <- sale_listings %>%
#   filter(addr_lat > 35,
#          addr_lon < -70, 
#          addr_lon > -76)
```

```{r simple decision tree with location and other variables}
# old tree model
# tree_all = rpart(price ~ bedrooms + bathrooms + size_sqft + time_to_subway + year_built, data = sale_listings_ss, subset = train, method = "class")
# fancyRpartPlot(tree_all)
```

```{r calculating accuracy of decision tree}
# price_prediction_tree <- predict(tree_all, sale_listings, type = 'class')
# # comparing predictions to actual prices
# table <- table(Price = sale_listings$price, Prediction = price_prediction_tree)
# # determining model accuracy
# accuracy_Test <- sum(diag(table)) / sum(table)
# print(paste('Accuracy for test', accuracy_Test))
```

```{r developing a random forest}
# # we can't include size_sqft + time_to_subway + year_built because there are missing values... can we impute/substitute values in???
# sale_listings_rf <- randomForest(price ~ bedrooms + bathrooms + addr_lat + addr_lon, data = sale_listings_adj, ntree = 10)
# price_prediction_rf <- predict(sale_listings_rf, sale_listings, type = "response")
# t <- table(Price = sale_listings$price, Prediction = price_prediction_rf)
# varImpPlot(sale_listings_rf)
```

```{r tried to mape ):}
# price_prediction_rf <- as.data.frame(price_prediction_rf)
# price_prediction_rf <- as.integer(price_prediction_rf$price_prediction_rf)
# MAPE(y_pred = price_prediction_rf, y_true = sale_listings$price)
# RMSE(y_pred = price_prediction_rf, y_true = sale_listings$price)
```


From here on out is the new decision tree analysis:

```{r simple baseline decision tree model with location and other variables}
# establishing test and training sets
train <- sale_listings %>% 
  sample_frac(.70)
test <- sale_listings %>% 
  anti_join(train)

tree_prelim = rpart(price ~ bedrooms + bathrooms + size_sqft, data = train, method = "anova")
fancyRpartPlot(tree_all)
```

```{r developing a random forest for baseline model}
# we can't include size_sqft + time_to_subway + year_built because there are missing values... can we impute/substitute values in???
sale_listings_rf <- randomForest(price ~ bedrooms + bathrooms, data = train, ntree = 10)

price_prediction_rf <- predict(sale_listings_rf, test, type = "response")

varImpPlot(sale_listings_rf)
```

```{r mape and rmse for baseline model}
price_prediction_rf <- as.data.frame(price_prediction_rf)
price_prediction_rf <- as.integer(price_prediction_rf$price_prediction_rf)
MAPE(y_pred = price_prediction_rf, y_true = test$price)
RMSE(y_pred = price_prediction_rf, y_true = test$price)
```
high mape ): yikes!

```{r adding new variables to improve baseline model}
set.seed(410)
train2 <- sale_listings_ss %>% sample_frac(.70)
test2 <- sale_listings_ss %>% anti_join(train2)


tree_all_ss = rpart(price ~ bedrooms + bathrooms + size_sqft + unittype + county + is_historic + floor_count + stainless_steel + hw_floors + wd + pet_friendly + city_group, data = train2, method = "anova", control = rpart.control(cp = 0.005))
fancyRpartPlot(tree_all_ss)

set.seed(410)
sale_listings_rf2 <- randomForest(price ~ bedrooms + bathrooms + size_sqft + unittype + county + is_historic + floor_count + city_group + stainless_steel + hw_floors + wd + pet_friendly, data = train2, ntree = 10)

#+ is_historic + floor_count + stainless_steel + hw_floors + wd + pet_friendly + city_group

test_rf2 <- test2 %>%
  mutate(price_hat = predict(sale_listings_rf2, test2))

varImpPlot(sale_listings_rf2)

MAPE(y_pred = test_rf2$price_hat, y_true = test2$price)
RMSE(test_rf2$price_hat, test2$price)


# bed
# bath
# size_sqft
# MAPE 0.6788141
# RMSE 686466.6

# unittype
# MAPE 0.5631888
# RMSE 596811.7

# county 
# MAPE 0.4291711
# RMSE 506354.3

# is_historic
# MAPE 0.3490612
# RMSE 454462.8

# floor_count
# MAPE 0.3242541
# RMSE 426788.2

# city_group 
# MAPE 0.3153056
# RMSE 427624.5

# stainless_steel 
# hw_floors 
# wd 
# pet_friendly
# MAPE 0.2860886
# RMSE 408977.7

```

```{r adding sentiment scores to model}
train3 <- sentiments_join %>% sample_frac(.70)
test3 <- sentiments_join %>% anti_join(train3)

tree_sentiments = rpart(price ~ bedrooms + bathrooms + size_sqft + unittype + county + is_historic + floor_count + stainless_steel + hw_floors + wd + pet_friendly + city_group + score, data = train3, method = "anova", control = rpart.control(cp = 0.005))
fancyRpartPlot(tree_sentiments)

sale_listings_rf3 <- randomForest(price ~ bedrooms + bathrooms + size_sqft + unittype + county + is_historic + floor_count + stainless_steel + hw_floors + wd + pet_friendly + city_group + score, data = train3, ntree = 10, na.action = na.omit) # Elaine & Emma think NAs are causing error

price_prediction_rf3 <- predict(sale_listings_rf3, test3, type = "response")
varImpPlot(sale_listings_rf3)

price_prediction_rf3 <- as.data.frame(price_prediction_rf3)
price_prediction_rf3 <- as.integer(price_prediction_rf3$price_prediction_rf3)
MAPE(y_pred = price_prediction_rf3, y_true = test3$price)
RMSE(y_pred = price_prediction_rf3, y_true = test3$price)
```

```{r adding sentiment scores to model}
#about 2000 listings does not have a score -- mainly bc their description does not contain any words associated with sentiments; will assign 0 
  
set.seed(410)
train3 <- sale_listings_ss %>% sample_frac(.70)
test3 <- sale_listings_ss %>% anti_join(train3)

tree_sentiments = rpart(price ~ bedrooms + bathrooms + size_sqft + unittype + county + is_historic + floor_count + stainless_steel + hw_floors + wd + pet_friendly + city_group + score, data = train3, method = "anova", control = rpart.control(cp = 0.005))

fancyRpartPlot(tree_sentiments)

set.seed(410)
sale_listings_rf3 <- randomForest(price ~ bedrooms + bathrooms + size_sqft + unittype + county + is_historic + floor_count + city_group + stainless_steel + hw_floors + wd + pet_friendly + score, data = train3, ntree = 10, na.action = na.omit) # Elaine & Emma think NAs are causing error

varImpPlot(sale_listings_rf3) 

test_rf3 <- test3 %>%
  mutate(price_hat = predict(sale_listings_rf3, test3))

MAPE(y_pred = test_rf3$price_hat, y_true = test3$price)
RMSE(test_rf3$price_hat, test3$price)

# sentiment score
# MAPE  0.2816914
# RMSE 405217.2
#I tried logprice and logsize but the RMSE for the same model went worse; probably we just go with raw price 

```

#model tuning 
```{r}
library(caret)
numFolds <- trainControl(method = "cv", number = 5)
tune.grid <- expand.grid(maxdepth = 2:25)
cpGrid <- expand.grid(.cp = seq(0.001, 0.3, 0.002))
mtry <- 2:20
tunegrid <- expand.grid(.mtry = mtry)

#select all variables in the model for model tuning - let decision tree decide which one is important to keep
sale_listings_tune <- sale_listings_ss %>%
  select(-id, -property_id, -listing_description, -size_sqft_na, -addr_street, -addr_unit, -zipcode, -year_built, -lat, -long, -major_city)

set.seed(410)
traint <- sale_listings_tune %>% sample_frac(.70)
testt <- sale_listings_tune %>% anti_join(traint)

#decision tree 
#rpart2 in caret takes in maxdepth
#max depth 8 for decision tree
set.seed(410)
rpart_tune <- caret::train(form = price~.,
                           data = traint,
                           method="rpart2",
                           tuneGrid = tune.grid,
                           trControl = numFolds,
                           metric = "RMSE")

depth_8 <- rpart(price ~., data = traint, method = "anova", control = rpart.control(maxdepth = 8))
fancyRpartPlot(depth_8)

#random forest 
#this take forever to run - the best model is mtry17 
set.seed(410)
best_rf <- caret::train(price ~ ., 
                          data = traint, 
                          method = "rf",
                          ntree = 100, 
                          trControl = numFolds,
                          metric = "RMSE",
                          tuneGrid = tunegrid,
                          na.action = na.omit)

#run the following model for mtry = 17
set.seed(410)
mtry <- 17
bestmtry <- expand.grid(.mtry = mtry)

#caret rf mtry = 17, ntree = 100, all vars
#RMSE: 383510.2
mtry_17 <- caret::train(price ~ ., 
                          data = traint, 
                          method = "rf",
                          tuneGrid = bestmtry,
                          ntree = 100, 
                          trControl = numFolds,
                          metric = "RMSE",
                          na.action = na.omit)

model_rf_17 <- randomForest(
  form = price~.,
  data = traint, 
  mtry = 17,
  ntree = 100)

varImpPlot(model_rf_17)

test_rf17 <- testt %>%
  mutate(price_hat = predict(model_rf_17, testt))


MAPE(y_pred = test_rf17$price_hat, y_true = testt$price)
RMSE(test_rf17$price_hat, testt$price)
#random forest mtry = 17, ntree = 100, all vars
#mape  0.2619716
#rmse 391404.6

summary(sale_listings$is_historic)
unique(sale_listings_imputed$is_historic)
summary(sentiments_join$is)
```



https://rstudio-pubs-static.s3.amazonaws.com/71575_4068e2e6dc3d46a785ad7886426c37db.html

https://topepo.github.io/caret/available-models.html